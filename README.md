# ğŸš€ Lunar Lander with Double DQN

A comprehensive Reinforcement Learning project implementing Double Deep Q-Network (Double DQN) to solve the Lunar Lander environment from OpenAI Gym.

## ğŸ¯ Project Overview

This project demonstrates the application of value-based reinforcement learning methods to solve the continuous control problem of landing a spacecraft. The agent learns to control the lander's thrusters to achieve smooth landings while minimizing fuel consumption.

### Key Features
- **Double DQN Implementation**: Reduces overestimation bias in Q-learning
- **Experience Replay**: Improves sample efficiency and stability
- **Target Network**: Stabilizes training by using separate target Q-network
- **Comprehensive Visualization**: Real-time training plots and landing animations
- **Model Persistence**: Save and load trained models
- **Performance Metrics**: Detailed analysis of learning progress

## ğŸ› ï¸ Technical Implementation

### Value-Based Method: Double DQN
- **Algorithm**: Double Deep Q-Network (van Hasselt et al., 2016)
- **Network Architecture**: Deep Neural Network with experience replay
- **Exploration Strategy**: Epsilon-greedy with decay
- **Optimization**: Adam optimizer with learning rate scheduling

### Environment Details
- **State Space**: 8-dimensional continuous space (position, velocity, angle, etc.)
- **Action Space**: 4 discrete actions (do nothing, fire left, fire main, fire right)
- **Reward Structure**: +100 for successful landing, penalties for crashes and fuel usage

## ğŸ“ Project Structure

```
lunar_lander_dqn/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Hyperparameters and settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/               # DQN and Double DQN implementations
â”‚   â”œâ”€â”€ models/              # Neural network architectures
â”‚   â”œâ”€â”€ utils/               # Utilities (replay buffer, plotting, etc.)
â”‚   â””â”€â”€ training/            # Training loop and logic
â”œâ”€â”€ main.py                  # Main training script
â”œâ”€â”€ evaluate.py              # Model evaluation script
â”œâ”€â”€ results/                 # Training results and plots
â”œâ”€â”€ saved_models/            # Trained model weights
â””â”€â”€ notebooks/               # Jupyter notebooks for analysis
```

## ğŸš€ Quick Start

### 1. Installation
```bash
pip install -r requirements.txt
```

### 2. Training
```bash
python main.py
```

### 3. Evaluation
```bash
python evaluate.py --model saved_models/best_model.pth
```

## ğŸ“Š Expected Results

The agent should achieve:
- **Episode Score**: 200+ (successful landing)
- **Training Time**: ~1000-2000 episodes
- **Success Rate**: >90% after training
- **Landing Precision**: Smooth, controlled landings

## ğŸ”¬ Research Background

This implementation is based on:
- **DQN**: Mnih et al. (2015) - Human-level control through deep reinforcement learning
- **Double DQN**: van Hasselt et al. (2016) - Deep reinforcement learning with double Q-learning
- **Experience Replay**: Lin (1992) - Self-improving reactive agents

## ğŸ“ˆ Performance Visualization

The project includes comprehensive visualization:
- Real-time training progress
- Episode rewards over time
- Loss curves
- Landing success rate
- Q-value distributions

## ğŸ¤ Contributing

This project is designed for educational purposes and research. Feel free to experiment with:
- Different network architectures
- Hyperparameter tuning
- Additional RL algorithms
- Environment modifications

## ğŸ“š References

1. van Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double Q-learning.
2. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning.
3. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.

## ğŸ† Project Outcomes

This project demonstrates:
- **Theoretical Understanding**: Implementation of state-of-the-art RL algorithms
- **Practical Application**: Real-world control problem solving
- **Research Skills**: Experimental design and result analysis
- **Software Engineering**: Professional code structure and documentation 